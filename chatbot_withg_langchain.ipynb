{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNPuKiHD22GHyr6hhGp1pbj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/deeplearning-sequences/blob/main/chatbot_withg_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.linkedin.com/pulse/get-insight-from-your-business-data-build-llm-application-jain/"
      ],
      "metadata": {
        "id": "Jy4kM8kSOVZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VA8F7JkANhKW"
      },
      "outputs": [],
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
        "!git clone https://huggingface.co/google/flan-t5-large"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install pypdf\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "jNZ2jGmmNkgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "pdfLoader = PyPDFLoader(\"/content/sample_data/large_language_models.pdf\")\n",
        "documents = pdfLoader.load()"
      ],
      "metadata": {
        "id": "AB91lDNqN7i8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "ZrzGl8IyS8MO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "modelPath = \"/content/all-MiniLM-L6-v2\"\n",
        "model_kwargs = {'device':'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings':False}\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "  model_name = modelPath,\n",
        "  model_kwargs = model_kwargs,\n",
        "  encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "E0DGNROnTK9P"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "db = FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "JHs-lIrCT6hU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are the variants of the transformer architectures used in LLMs?\"\n",
        "searchDocs = db.similarity_search(question)\n",
        "print(searchDocs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJyWPD0oTVuE",
        "outputId": "6c5e1ce6-25e2-4c32-ca7b-1a479664ead2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "where Xis the input of layer and l,W, b, V andcare learned\n",
            "parameters.\n",
            "Other GLU variants [74] used in LLMs are:\n",
            "ReGLU (x, W, V, b, c ) =max(0, xW +b)⊗,\n",
            "GEGLU (x, W, V, b, c ) =GELU (xW+b)⊗(xV+c),\n",
            "SwiGLU (x, W, V, b, c, β ) =Swishβ (xW+b)⊗(xV+c).\n",
            "E. Layer Normalization\n",
            "Layer normalization leads to faster convergence and is an\n",
            "integrated component of transformers [64]. Besides Layer-\n",
            "Norm [76] and RMSNorm [77], LLMs use pre-layer normal-\n",
            "ization [78], applying it before multi-head attention (MHA).\n",
            "Pre-norm is shown to provide training stability in LLMs.\n",
            "Another normalization variant, DeepNorm [79] fixes larger\n",
            "gradient issue in pre-norm.\n",
            "F . Distributed LLM Training\n",
            "This section describes distributed LLM training approaches\n",
            "briefly. More details are available in [13], [37], [80], [81].\n",
            "Data Parallelism: Data parallelism replicates the model on\n",
            "multiple devices where data in a batch gets divided across\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/flan-t5-large\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"/content/flan-t5-large\")\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline = pipe,\n",
        "    model_kwargs={\"temperature\": 0, \"max_length\": 1512, \"max_new_tokens\" : 1500},\n",
        ")"
      ],
      "metadata": {
        "id": "KytXRcwwTmw9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n"
      ],
      "metadata": {
        "id": "F0eJVgRIUJwA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "  llm=llm,\n",
        "  chain_type=\"stuff\",\n",
        "  retriever=db.as_retriever(),\n",
        "  chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")\n"
      ],
      "metadata": {
        "id": "lafcbHU4VKnE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY-0Hqc0VMaE",
        "outputId": "babff27e-7154-42b8-9a78-c2b043bae684"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible. \\n{context}\\nQuestion: {question}\\nHelpful Answer:\"), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text2text_generation.Text2TextGenerationPipeline object at 0x7b8bd506dc30>, model_kwargs={'temperature': 0, 'max_length': 1512, 'max_new_tokens': 1500})), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7b8c1da48460>))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain({ \"query\" : question })\n",
        "print(result[\"result\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK5HCyZRUvoB",
        "outputId": "b79e6998-e5c9-4ca2-f86b-49f3a2c2dbfb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1038 > 512). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The difference arises due to the application of the attention and the connection of transformer blocks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kfN7akTYWvC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KAGhwmOPU2PI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}