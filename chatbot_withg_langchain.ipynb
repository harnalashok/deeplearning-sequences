{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlN5BpatibOSfINpfAnTM5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/deeplearning-sequences/blob/main/chatbot_withg_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.linkedin.com/pulse/get-insight-from-your-business-data-build-llm-application-jain/"
      ],
      "metadata": {
        "id": "Jy4kM8kSOVZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VA8F7JkANhKW",
        "outputId": "75fed75a-8643-4977-ad59-8651dbb803ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'all-MiniLM-L6-v2'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 52 (delta 2), reused 0 (delta 0), pack-reused 46\u001b[K\n",
            "Unpacking objects: 100% (52/52), 317.58 KiB | 2.06 MiB/s, done.\n",
            "Filtering content: 100% (3/3), 260.15 MiB | 50.87 MiB/s, done.\n",
            "Cloning into 'flan-t5-large'...\n",
            "remote: Enumerating objects: 110, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 110 (delta 0), reused 0 (delta 0), pack-reused 107\u001b[K\n",
            "Receiving objects: 100% (110/110), 635.37 KiB | 1.42 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n",
            "Filtering content: 100% (5/5), 11.91 GiB | 43.50 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
        "!git clone https://huggingface.co/google/flan-t5-large"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install pypdf\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "jNZ2jGmmNkgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "pdfLoader = PyPDFLoader(\"/content/sample_data/large_language_models.pdf\")\n",
        "documents = pdfLoader.load()"
      ],
      "metadata": {
        "id": "AB91lDNqN7i8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "ZrzGl8IyS8MO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "modelPath = \"/content/all-MiniLM-L6-v2\"\n",
        "model_kwargs = {'device':'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings':False}\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "  model_name = modelPath,\n",
        "  model_kwargs = model_kwargs,\n",
        "  encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "E0DGNROnTK9P"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "db = FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "JHs-lIrCT6hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Discuss the variants of the transformer architectures used in LLMs\"\n",
        "searchDocs = db.similarity_search(question)\n",
        "print(searchDocs[0].page_content)"
      ],
      "metadata": {
        "id": "NJyWPD0oTVuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/flan-t5-large\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"/content/flan-t5-large\")\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline = pipe,\n",
        "    model_kwargs={\"temperature\": 0, \"max_length\": 1512, \"max_new_tokens\" : 1500},\n",
        ")"
      ],
      "metadata": {
        "id": "KytXRcwwTmw9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n"
      ],
      "metadata": {
        "id": "F0eJVgRIUJwA"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "  llm=llm,\n",
        "  chain_type=\"stuff\",\n",
        "  retriever=db.as_retriever(),\n",
        "  chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")\n"
      ],
      "metadata": {
        "id": "lafcbHU4VKnE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY-0Hqc0VMaE",
        "outputId": "3e2bfc0a-a4f9-4665-ef78-0249933b5f75"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible. \\n{context}\\nQuestion: {question}\\nHelpful Answer:\"), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text2text_generation.Text2TextGenerationPipeline object at 0x7f89ce4903d0>, model_kwargs={'temperature': 0, 'max_length': 1512, 'max_new_tokens': 1500})), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f8af181b760>))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain({ \"query\" : question })\n",
        "print(result[\"result\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK5HCyZRUvoB",
        "outputId": "92c73c29-5654-40e6-86c2-31ef617e1972"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1036 > 512). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here we discuss the variants of the transformer architectures used in LLMs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kfN7akTYWvC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KAGhwmOPU2PI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}