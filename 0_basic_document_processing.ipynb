{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0.basic document processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMh7GnB0uOPZ/ydgG5TdZhu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/deeplearning-sequences/blob/main/0_basic_document_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8l27MZ8pkSr"
      },
      "source": [
        "# Last amended: 07th March, 2021\r\n",
        "# Ref: https://radimrehurek.com/gensim/tut1.html#from-strings-to-vectors\r\n",
        "#      https://www.tutorialspoint.com/gensim/gensim_creating_a_bag_of_words_corpus.htm\r\n",
        "#\r\n",
        "# Objective(s):\r\n",
        "#         A. Familiarising with Document processing\r\n",
        "#            using gensim.\r\n",
        "#         B. Convert tokens with each document to corresponding\r\n",
        "#            'token-ids' or integer-tokens.\r\n",
        "#            (For text cleaning, pl refer wikiclustering file\r\n",
        "#            in folder: 10.nlp_workshop/text_clustering)\r\n",
        "#            (Keras also has  Tokenizer class that can also be\r\n",
        "#            used for integer-tokenization. See file:\r\n",
        "#            8.rnn/3.keras_tokenizer_class.py\r\n",
        "#            nltk can also tokenize. See file:\r\n",
        "#            10.nlp_workshop/word2vec/nlp_workshop_word2vec.py)\r\n",
        "#         C. Creating a Bag-of-words model\r\n",
        "#         D. Discovering document similarity\r\n",
        "\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6foXDbN1Buy"
      },
      "source": [
        "The core concepts are:  \r\n",
        "**Document**  \r\n",
        "A document is an object of the text sequence type (commonly known as str in Python 3). A document could be anything from a short 140 character tweet, a single paragraph (i.e., journal article abstract), a news article, or a book.  \r\n",
        "`document = \"Human machine interface for lab abc computer applications\"`\r\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYogxQOG--X5"
      },
      "source": [
        "**Corpus**  \r\n",
        "A corpus is a collection of Document objects. Corpora serve two roles in Gensim:\r\n",
        "1.  Input for training a Model. During training, the models use this training corpus to look for common themes and topics, initializing their internal model parameters.\r\n",
        "\r\n",
        "2.  Gensim focuses on unsupervised models so that no human intervention, such as costly annotations or tagging documents by hand, is required.\r\n",
        "\r\n",
        "3. Documents to organize. After training, a topic model can be used to extract topics from new documents (documents not seen in the training corpus).\r\n",
        "\r\n",
        "4.  Such corpora can be indexed for Similarity Queries, queried by semantic similarity, clustered etc.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "ByawbP_e-4HS",
        "outputId": "97886d9f-b096-4b3b-bf79-a5e52d47c6c6"
      },
      "source": [
        "#0.0 Upgrade existing gensim\r\n",
        "!pip install --upgrade gensim"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/afe2315e08a38967f8a3036bbe7e38b428e9b7a90e823a83d0d49df1adf5/gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 45.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (4.2.0)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.8.0\n",
            "    Uninstalling gensim-3.8.0:\n",
            "      Successfully uninstalled gensim-3.8.0\n",
            "Successfully installed gensim-3.8.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gensim"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "w507D0iwpoaj",
        "outputId": "0531b30d-b0c4-4205-da9c-da2aa195b277"
      },
      "source": [
        "## Call libraries\r\n",
        "\r\n",
        "%reset -f\r\n",
        "\r\n",
        "# 1.1  gensim contains tools for Natural Language Processing\r\n",
        "#      Module 'corpora' contains sub-modules and methods to\r\n",
        "#      work with text documents\r\n",
        "from gensim import corpora\r\n",
        "\r\n",
        "# 1.2 defaultdict is like an ordinary dict. Only that if a key does\r\n",
        "#     not exist in the dict, then on its search it inserts that 'key'\r\n",
        "#     (as if that key existed)with a value that is defined by an \r\n",
        "#     initialization function (such as int()). \r\n",
        "#     See at the end of code: 'Dictionaries in Python'\r\n",
        "\r\n",
        "from collections import defaultdict\r\n",
        "\r\n",
        "# 1.3\r\n",
        "from gensim.utils import simple_preprocess\r\n",
        "\r\n",
        "# 1.4 To unnest a list of lists\r\n",
        "from gensim.utils import flatten\r\n",
        "\r\n",
        "# 1.5 pprint does pretty printing\r\n",
        "import pprint\r\n",
        "\r\n",
        "# 1.6\r\n",
        "import gensim\r\n",
        "gensim.__version__\r\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.8.3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OWqnbhaC4Ss"
      },
      "source": [
        "# 1.7 Display outputs of multiple commands from a cell:\r\n",
        "\r\n",
        "from IPython.core.interactiveshell import InteractiveShell\r\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\r\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vac3GFTGPVZl"
      },
      "source": [
        "### Preprocessing and tokenizing corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pvLF45lSHiG"
      },
      "source": [
        "What is a Document  \r\n",
        "\r\n",
        "A document is an object of the text sequence type (commonly known as str in Python 3). A document could be anything from a short 140 character tweet, a single paragraph (i.e., journal article abstract), a news article, or a book.\r\n",
        "`document = \"Human machine interface for lab abc computer applications\"`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vFTj8pdSctd"
      },
      "source": [
        "**Corpus**  \r\n",
        "A corpus is a collection of Document objects. Corpora serve two roles in Gensim:\r\n",
        "1.  Input for training a Model. During training, the models use this training corpus to look for common themes and topics, initializing their internal model parameters.\r\n",
        "\r\n",
        "2.  Gensim focuses on unsupervised models so that no human intervention, such as costly annotations or tagging documents by hand, is required.\r\n",
        "\r\n",
        "3. Documents to organize. After training, a topic model can be used to extract topics from new documents (documents not seen in the training corpus).\r\n",
        "\r\n",
        "4.  Such corpora can be indexed for Similarity Queries, queried by semantic similarity, clustered etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REiBOguLpt2e"
      },
      "source": [
        "# 2. Here is an example corpus. It consists of \r\n",
        "#    9 documents, where each document is a string\r\n",
        "#    consisting of a single sentence. \r\n",
        "#    Create a sample collection (list) of documents\r\n",
        "#    See text_clustering.py file as to how to get this list\r\n",
        "#    from folder of files or pandas dataframe:\r\n",
        "\r\n",
        "#    The first string is a paragraph having 3 sentences\r\n",
        "\r\n",
        "text_corpus = [ \r\n",
        "                \"Human machine interface for lab computer applications. Use it at abc. OK.\",\r\n",
        "                \"A survey at of user opinion of computer system response time\",\r\n",
        "                \"The EPS user interface management system\",\r\n",
        "                \"System and human system engineering testing of EPS\",\r\n",
        "                \"Relation of user perceived response time to error measurement\",\r\n",
        "                \"The generation of random binary unordered trees\",\r\n",
        "                \"The intersection graph of paths in trees\",\r\n",
        "                \"Graph minors IV Widths of trees and well at quasi ordering\",\r\n",
        "                \"Graph minors A survey\"\r\n",
        "                ]\r\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kIKbm8SAPIv"
      },
      "source": [
        "Note  \r\n",
        "The above example loads the entire corpus into memory. In practice, corpora may be very large, so loading them into memory may be impossible. Gensim intelligently handles such corpora by streaming them one document at a time. See Corpus Streaming – One Document at a Time for details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbBMgnFdA4yA"
      },
      "source": [
        "Pre-processing steps:   \r\n",
        "\r\n",
        "After collecting our corpus, there are typically a number of preprocessing steps we want to undertake. We’ll keep it simple and just remove some commonly used English words (such as ‘the’) and words that occur only once in the corpus. In the process of doing so, we’ll tokenize our data. Tokenization breaks up the documents into words (in this case using space as a delimiter)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "craIWA2rBAAY"
      },
      "source": [
        "# 2.1 Clean documents: See file text_clustering.py\r\n",
        "# 2.2 Stem documents : See file text_clustering.py"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE0ZhnOEp_4C"
      },
      "source": [
        "# 2.3 Lowercase each document, split it by white space and filter out stopwords\r\n",
        "\r\n",
        "# 2.3.1\r\n",
        "#     Create some list of stopwords that we do not want\r\n",
        "#     Detailed list of english stopwords is available at:\r\n",
        "#     https://gist.github.com/sebleier/554280\r\n",
        "\r\n",
        "#     We are not including the word 'at' here:\r\n",
        "\r\n",
        "stoplist = set('for a of the and to in'.split())\r\n"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZH4Q3hDqHAt"
      },
      "source": [
        "# 3. Tokenize--I\r\n",
        "#    Define our own tokenize function.\r\n",
        "#     This function parses list of strings into\r\n",
        "#      list of words for each element or document\r\n",
        "#       in the document-collection\r\n",
        "def tokenize(docs):\r\n",
        "    tokenized = []          # Ist List: This will be a list of lists\r\n",
        "    for document in docs:   # For each senetence in the document-collection\r\n",
        "        tokenized_document = []  # IInd list: List of words per string or document\r\n",
        "        for word in document.lower().split():\r\n",
        "            if word not in stoplist:\r\n",
        "                tokenized_document.append(word)  # Append it to a list\r\n",
        "        tokenized.append(tokenized_document)         # Append list of words to a list\r\n",
        "    return tokenized\r\n"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CeyP3-gqMVN",
        "outputId": "b0236713-e7fd-4df0-f99e-2f96d8698d7f"
      },
      "source": [
        "# 3.1 Apply the above function\r\n",
        "#     to tokenize (parse) the corpus:\r\n",
        "\r\n",
        "texts = tokenize(text_corpus)\r\n",
        "\r\n",
        "#3.1.1 \r\n",
        "pprint.pprint(texts)   #  List of list. The inner list\r\n",
        "                       #  contains tokens of respective documents\r\n",
        "len(texts)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['human',\n",
            "  'machine',\n",
            "  'interface',\n",
            "  'lab',\n",
            "  'computer',\n",
            "  'applications.',\n",
            "  'use',\n",
            "  'it',\n",
            "  'at',\n",
            "  'abc.',\n",
            "  'ok.'],\n",
            " ['survey', 'at', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
            " ['eps', 'user', 'interface', 'management', 'system'],\n",
            " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
            " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
            " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
            " ['intersection', 'graph', 'paths', 'trees'],\n",
            " ['graph',\n",
            "  'minors',\n",
            "  'iv',\n",
            "  'widths',\n",
            "  'trees',\n",
            "  'well',\n",
            "  'at',\n",
            "  'quasi',\n",
            "  'ordering'],\n",
            " ['graph', 'minors', 'survey']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMYkkoiSw6Jx",
        "outputId": "46404103-35ef-4f64-dc90-0ef0aafbead4"
      },
      "source": [
        "# 3.1.2 Should you like, you can \r\n",
        "#       unnest the list of lists:\r\n",
        "\r\n",
        "ft = flatten(texts)\r\n",
        "print(ft)\r\n",
        "vocab = set(ft)\r\n",
        "print(\"\\n---Vocab-----\\n\")\r\n",
        "print(vocab)\r\n",
        "print(\"\\n---str length-----\\n\")\r\n",
        "len(ft)   # 55\r\n",
        "print(\"\\n---vocab length-----\\n\")\r\n",
        "len(vocab)  # 36"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['human', 'machine', 'interface', 'lab', 'computer', 'applications.', 'use', 'it', 'at', 'abc.', 'ok.', 'survey', 'at', 'user', 'opinion', 'computer', 'system', 'response', 'time', 'eps', 'user', 'interface', 'management', 'system', 'system', 'human', 'system', 'engineering', 'testing', 'eps', 'relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement', 'generation', 'random', 'binary', 'unordered', 'trees', 'intersection', 'graph', 'paths', 'trees', 'graph', 'minors', 'iv', 'widths', 'trees', 'well', 'at', 'quasi', 'ordering', 'graph', 'minors', 'survey']\n",
            "\n",
            "---Vocab-----\n",
            "\n",
            "{'lab', 'quasi', 'testing', 'widths', 'human', 'graph', 'measurement', 'at', 'well', 'engineering', 'random', 'relation', 'machine', 'perceived', 'interface', 'applications.', 'use', 'minors', 'binary', 'opinion', 'response', 'time', 'management', 'error', 'unordered', 'system', 'paths', 'abc.', 'trees', 'generation', 'user', 'eps', 'intersection', 'ok.', 'it', 'iv', 'computer', 'survey', 'ordering'}\n",
            "\n",
            "---str length-----\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---vocab length-----\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOQFGKeMqTwQ",
        "outputId": "2c6c6af2-9654-4ba6-9a9d-4362bbf17948"
      },
      "source": [
        "# 3.2 Tokenize--II\r\n",
        "#     The following code is equivalent to above nested for-loops\r\n",
        "#     There being one list comprehension between another, output\r\n",
        "#     is not one list (as in ordinary list comprehension) but list\r\n",
        "#     within list.\r\n",
        "\r\n",
        "texts = [\r\n",
        "         [word  for word in document.lower().split(' ') if word not in stoplist]\r\n",
        "         for document in text_corpus \r\n",
        "        ]\r\n",
        "\r\n",
        "# 3.2.1\r\n",
        "texts"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['human',\n",
              "  'machine',\n",
              "  'interface',\n",
              "  'lab',\n",
              "  'computer',\n",
              "  'applications.',\n",
              "  'use',\n",
              "  'it',\n",
              "  'at',\n",
              "  'abc.',\n",
              "  'ok.'],\n",
              " ['survey', 'at', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
              " ['eps', 'user', 'interface', 'management', 'system'],\n",
              " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
              " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
              " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
              " ['intersection', 'graph', 'paths', 'trees'],\n",
              " ['graph',\n",
              "  'minors',\n",
              "  'iv',\n",
              "  'widths',\n",
              "  'trees',\n",
              "  'well',\n",
              "  'at',\n",
              "  'quasi',\n",
              "  'ordering'],\n",
              " ['graph', 'minors', 'survey']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgRdTFHwRsvR",
        "outputId": "1167c662-2e90-4866-90b2-3423cafda93d"
      },
      "source": [
        "# 3.3 Tokenize--III\r\n",
        "#     Convert a document into a list of lowercase tokens,\r\n",
        "#     ignoring tokens that are too short or too long.\r\n",
        "#     Uses tokenize() internally.\r\n",
        "\r\n",
        "my_texts = []\r\n",
        "\r\n",
        "for doc in text_corpus:\r\n",
        "   inner = []\r\n",
        "   out = simple_preprocess(doc, min_len=3, max_len=15)\r\n",
        "   my_texts.append(out)\r\n",
        "\r\n",
        "my_texts    "
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['human',\n",
              "  'machine',\n",
              "  'interface',\n",
              "  'for',\n",
              "  'lab',\n",
              "  'computer',\n",
              "  'applications',\n",
              "  'use',\n",
              "  'abc'],\n",
              " ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
              " ['the', 'eps', 'user', 'interface', 'management', 'system'],\n",
              " ['system', 'and', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
              " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
              " ['the', 'generation', 'random', 'binary', 'unordered', 'trees'],\n",
              " ['the', 'intersection', 'graph', 'paths', 'trees'],\n",
              " ['graph', 'minors', 'widths', 'trees', 'and', 'well', 'quasi', 'ordering'],\n",
              " ['graph', 'minors', 'survey']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICJnfFMGTaLU"
      },
      "source": [
        "#### Counting frequency of occurrence\r\n",
        "\r\n",
        "Once a corpus has been tokenized, we can beging counting frequency of occurrence within a corpus. As we will see below, vectorization process does it automatically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNUueHokqXjP"
      },
      "source": [
        "# 4.\r\n",
        "# Ref : https://www.ludovf.net/blog/python-collections-defaultdict/\r\n",
        "\r\n",
        "#  A defaultdict is just like a regular Python dict,\r\n",
        "#  except that it supports an additional argument at\r\n",
        "#  initialization: a function. If someone attempts to\r\n",
        "#  access a key to which no value has been assigned,\r\n",
        "#  that function will be called (without arguments)\r\n",
        "#  and its return value is used as the default value\r\n",
        "#  for the key.\r\n"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfR9hVOWqa0o",
        "outputId": "36c0ca80-00d6-407e-e91d-ecc86659b9f0"
      },
      "source": [
        "# 4.1 Initialise and create an empty dictionary\r\n",
        "#     by name of 'frequency'\r\n",
        "\r\n",
        "# 4.1.1\r\n",
        "int()          # This function gives 0.\r\n",
        "               # Use it in defaultdict\r\n",
        "# 4.1.2               \r\n",
        "frequency = defaultdict(int)   # defaultdict(int) => key-values are int\r\n",
        "                               # defaultdict(list) => key-values are lists\r\n",
        "                               # Example: {'a' :['xx','yy'], 'b':['zz']}\r\n",
        "\r\n",
        "# 4.1.3                               \r\n",
        "frequency"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AinFJrDxqgAx"
      },
      "source": [
        "# 4.2 Get count of each word in the 'documents'\r\n",
        "#     for every list in lists\r\n",
        "\r\n",
        "for doc in texts:        \r\n",
        "    # for every word in the inner list  \r\n",
        "    for token in doc:\r\n",
        "    \t# frequency[token] will first add a key 'token' to dict\r\n",
        "    \t#  (if the 'key' does not already exit) holding value '0'.\r\n",
        "    \t#   In either case value of the key will be incremented by 1\r\n",
        "    \t# So after all the loop is completed, value of each key\r\n",
        "    \t# will show its frequency\r\n",
        "        frequency[token] += 1\r\n"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpD6BJKKqjad",
        "outputId": "46f39546-96ae-4c50-b34d-74aac36a7db7"
      },
      "source": [
        "# 4.2.1\r\n",
        "print(frequency)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<class 'int'>, {'human': 2, 'machine': 1, 'interface': 2, 'lab': 1, 'computer': 2, 'applications.': 1, 'use': 1, 'it': 1, 'at': 3, 'abc.': 1, 'ok.': 1, 'survey': 2, 'user': 3, 'opinion': 1, 'system': 4, 'response': 2, 'time': 2, 'eps': 2, 'management': 1, 'engineering': 1, 'testing': 1, 'relation': 1, 'perceived': 1, 'error': 1, 'measurement': 1, 'generation': 1, 'random': 1, 'binary': 1, 'unordered': 1, 'trees': 3, 'intersection': 1, 'graph': 3, 'paths': 1, 'minors': 2, 'iv': 1, 'widths': 1, 'well': 1, 'quasi': 1, 'ordering': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz5_2zhDql-w"
      },
      "source": [
        "# 4.3 Remove words that appear only once\r\n",
        "#     So we create another list of lists\r\n",
        "#     texts = [['he','he','to'],['to','go']]\r\n",
        "#     frequency={'he' : 2, 'to': 2, 'go': 1}\r\n",
        "\r\n",
        "# 4.3.1\r\n",
        "processed_corpus = []       # outer list\r\n",
        "\r\n",
        "# 4.3.2 For every list in the the\r\n",
        "#       outer list \r\n",
        "for doc in texts:\r\n",
        "  #  4.3.3 A blank list of tokens\r\n",
        "  #        with higher frequency\r\n",
        "\ttokens = [] \r\n",
        "\r\n",
        "  # 4.3.4 For every word in this \r\n",
        "  #       inner list   \r\n",
        "\tfor word in doc:\r\n",
        "\t\tif frequency[word] > 1:\r\n",
        "\t\t\ttokens.append(word)\r\n",
        " # 4.3.5\r\n",
        "\tprocessed_corpus.append(tokens)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uo9dqnpPQtK",
        "outputId": "15bb0138-26ed-43b9-d4d5-097b08e61577"
      },
      "source": [
        "# 4.3.6 Processed output\r\n",
        "processed_corpus"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['human', 'interface', 'computer', 'at'],\n",
              " ['survey', 'at', 'user', 'computer', 'system', 'response', 'time'],\n",
              " ['eps', 'user', 'interface', 'system'],\n",
              " ['system', 'human', 'system', 'eps'],\n",
              " ['user', 'response', 'time'],\n",
              " ['trees'],\n",
              " ['graph', 'trees'],\n",
              " ['graph', 'minors', 'trees', 'at'],\n",
              " ['graph', 'minors', 'survey']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xodqzno_JrmF",
        "outputId": "272ac417-1334-47c4-f90a-fc42ad23b39a"
      },
      "source": [
        "# 4.4 The above is equivalent to the following:\r\n",
        "#     Only keep words that appear more than once\r\n",
        "\r\n",
        "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\r\n",
        "pprint.pprint(processed_corpus)\r\n",
        "print(\"\\n-----Compare-----\\n\")\r\n",
        "print(texts)              # Compare the above with this"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['human', 'interface', 'computer', 'at'],\n",
            " ['survey', 'at', 'user', 'computer', 'system', 'response', 'time'],\n",
            " ['eps', 'user', 'interface', 'system'],\n",
            " ['system', 'human', 'system', 'eps'],\n",
            " ['user', 'response', 'time'],\n",
            " ['trees'],\n",
            " ['graph', 'trees'],\n",
            " ['graph', 'minors', 'trees', 'at'],\n",
            " ['graph', 'minors', 'survey']]\n",
            "\n",
            "-----Compare-----\n",
            "\n",
            "[['human', 'machine', 'interface', 'lab', 'computer', 'applications.', 'use', 'it', 'at', 'abc.', 'ok.'], ['survey', 'at', 'user', 'opinion', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'management', 'system'], ['system', 'human', 'system', 'engineering', 'testing', 'eps'], ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'], ['generation', 'random', 'binary', 'unordered', 'trees'], ['intersection', 'graph', 'paths', 'trees'], ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'at', 'quasi', 'ordering'], ['graph', 'minors', 'survey']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hjAqzUuPxrE"
      },
      "source": [
        "### Associating word with ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGhwsNqoIoER"
      },
      "source": [
        "Before proceeding further, we want to associate each word in the corpus with a unique integer ID. We can do this using the gensim.corpora.Dictionary class. This dictionary defines the vocabulary of all words that our processing knows about."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEMkFL5TqsK-"
      },
      "source": [
        "# 5. Module 'corpora.Dictionary' implements the concept of\r\n",
        "#     Dictionary – a mapping between words and their integer ids.\r\n",
        "#    Ref: https://radimrehurek.com/gensim/corpora/dictionary.html\r\n",
        "\r\n",
        "dictionary = corpora.Dictionary(processed_corpus)\r\n"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2voEDOHqvDw",
        "outputId": "d5ab3324-95f6-4466-f3a1-d110f3083547"
      },
      "source": [
        "# 5.1\r\n",
        "dictionary      # Just informs where it is stroed in memory\r\n",
        "\r\n",
        "\r\n",
        "# 5.1.1\r\n",
        "print(\"\\n\\n---Words and corresponding IDs------\\n\")\r\n",
        "list(dictionary.values())\r\n",
        "\r\n",
        "# 5.1.2\r\n",
        "print(\"\\n\\n--------------\\n\")\r\n",
        "dictionary.keys()\r\n",
        "\r\n",
        "\r\n",
        "# Id of 'computer' is 1\r\n",
        "# Id of 'human' is 2\r\n",
        "# Id of 'system' is 6\r\n",
        "# Id of 'minors' is 12\r\n",
        "# Word 'interaction' is absent"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.corpora.dictionary.Dictionary at 0x7fe8d95f1e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "---Words and corresponding IDs------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['at',\n",
              " 'computer',\n",
              " 'human',\n",
              " 'interface',\n",
              " 'response',\n",
              " 'survey',\n",
              " 'system',\n",
              " 'time',\n",
              " 'user',\n",
              " 'eps',\n",
              " 'trees',\n",
              " 'graph',\n",
              " 'minors']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "--------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQw49eTdDs_p"
      },
      "source": [
        "### Vectorization--Get bag of words\r\n",
        "\r\n",
        "To infer the latent structure in our corpus we need a way to represent documents that we can manipulate mathematically. One approach is to represent each document as a vector of features. For example, a single feature may be thought of as a question-answer pair:\r\n",
        "\r\n",
        "1. How many times does the word splonge appear in the document? Zero.\r\n",
        "2. How many paragraphs does the document consist of? Two.\r\n",
        "3. How many fonts does the document use? Five.\r\n",
        "\r\n",
        "#### One way\r\n",
        "\r\n",
        "The question is usually represented only by its integer id (such as 1, 2 and 3). The representation of this document then becomes a series of pairs like (1, 0.0), (2, 2.0), (3, 5.0). This is known as a dense vector, because it contains an explicit answer to each of the above questions.\r\n",
        "\r\n",
        "If we know all the questions in advance, we may leave them implicit and simply represent the document as (0, 2, 5). This sequence of answers is the vector for our document (in this case a 3-dimensional dense vector). For practical purposes, only questions to which the answer is (or can be converted to) a single floating point number are allowed in Gensim.\r\n",
        "\r\n",
        "In practice, vectors often consist of many zero values. To save memory, Gensim omits all vector elements with value 0.0. The above example thus becomes (2, 2.0), (3, 5.0). This is known as a sparse vector or bag-of-words vector. The values of all missing features in this sparse representation can be unambiguously resolved to zero, 0.0.\r\n",
        "\r\n",
        "Assuming the questions are the same, we can compare the vectors of two different documents to each other. For example, assume we are given two vectors (0.0, 2.0, 5.0) and (0.1, 1.9, 4.9). Because the vectors are very similar to each other, we can conclude that the documents corresponding to those vectors are similar, too. Of course, the correctness of that conclusion depends on how well we picked the questions in the first place.\r\n",
        "\r\n",
        "#### Bag of words\r\n",
        "Another approach to represent a document as a vector is the bag-of-words model. Under the bag-of-words model each document is represented by a vector containing the frequency counts of each word in the dictionary. For example, assume we have a dictionary containing the words ['coffee', 'milk', 'sugar', 'spoon']. A document consisting of the string \"coffee milk coffee\" would then be represented by the vector [2, 1, 0, 0] where the entries of the vector are (in order) the occurrences of “coffee”, “milk”, “sugar” and “spoon” in the document. The length of the vector is the number of entries in the dictionary. One of the main properties of the bag-of-words model is that it completely ignores the order of the tokens in the document that is encoded, which is where the name bag-of-words comes from.\r\n",
        "\r\n",
        "Our processed corpus has 13 unique words in it, which means that each document will be represented by a 13-dimensional vector under the bag-of-words model. We can use the dictionary to turn tokenized documents into these 13-dimensional vectors. We can see what these IDs correspond to:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxJoa5_3FriU"
      },
      "source": [
        "Our processed corpus has 13 unique words in it, which means that each document will be represented by a 13-dimensional vector under the bag-of-words model. We can use the dictionary to turn tokenized documents into these 13-dimensional vectors. We can see what these IDs correspond to:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojv-mvGYqxOt",
        "outputId": "2b48945a-b320-4a63-ef3b-ebaccceffa39"
      },
      "source": [
        "# 5.2 Another way to read word-Id pairs\r\n",
        "print(dictionary.token2id)      # Another function is id2token\r\n",
        " "
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'at': 0, 'computer': 1, 'human': 2, 'interface': 3, 'response': 4, 'survey': 5, 'system': 6, 'time': 7, 'user': 8, 'eps': 9, 'trees': 10, 'graph': 11, 'minors': 12}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I-mV5QQHiiL"
      },
      "source": [
        "For example, suppose we wanted to vectorize the phrase “Human computer interaction” (note that this phrase was not in our original corpus). We can create the bag-of-word representation for a document using the doc2bow method of the dictionary, which returns a sparse representation of the word counts:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtafH7cjUN1D"
      },
      "source": [
        "#### Vectorize an arbitrary document\r\n",
        "\r\n",
        "Based upon our availabe corpus, vectorize any arbitrary document.  \r\n",
        "\r\n",
        "For example, suppose we wanted to vectorize the phrase “Human computer interaction” (note that this phrase was not in our original corpus). We can create the bag-of-word representation for a document using the `doc2bow` method of the dictionary, which returns a sparse representation of the word counts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ho4XCOvVHmWk",
        "outputId": "f58a4e90-575f-4755-d888-b0099b0e5416"
      },
      "source": [
        "# 6.0 Transform to BOW\r\n",
        "new_doc = \"Human computer to computer to computer interaction\"\r\n",
        "new_vec = dictionary.doc2bow(new_doc.lower().split())\r\n",
        "print(new_vec)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1, 3), (2, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogGLLZw0HtIE"
      },
      "source": [
        "The first entry in each tuple corresponds to the ID of the token in the dictionary, the second corresponds to the count of this token.   \r\n",
        "\r\n",
        "Note that “interaction” did not occur in the original corpus and so it was not included in the vectorization. Also note that this vector only contains entries for words that actually appeared in the document. Because any given document will only contain a few words out of the many words in the dictionary, words that do not appear in the vectorization are represented as implicitly zero as a space saving measure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD_UPIAJH8nj"
      },
      "source": [
        "We can convert our entire original corpus to a list of vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSmJOfFcqzz5",
        "outputId": "71347e22-e94b-4e4b-cf47-5f3ba652cf38"
      },
      "source": [
        "# 6.1 Convert document into the bag-of-words (bow)\r\n",
        "#      format ie list of (integer-tokens, token_count) per document.\r\n",
        "#      Bag of sequences does not give integer sequences in the order\r\n",
        "#      they are in the sentence but in increasing order of integer values.\r\n",
        "#      Thus, it is just a bag.\r\n",
        "\r\n",
        "bag = [dictionary.doc2bow(text) for text in processed_corpus]\r\n",
        "bag\r\n"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
              " [(0, 1), (1, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
              " [(3, 1), (6, 1), (8, 1), (9, 1)],\n",
              " [(2, 1), (6, 2), (9, 1)],\n",
              " [(4, 1), (7, 1), (8, 1)],\n",
              " [(10, 1)],\n",
              " [(10, 1), (11, 1)],\n",
              " [(0, 1), (10, 1), (11, 1), (12, 1)],\n",
              " [(5, 1), (11, 1), (12, 1)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTQq_8ioIeP1"
      },
      "source": [
        "#### Document vs Vector   \r\n",
        "\r\n",
        "The distinction between a document and a vector is that the former is text, and the latter is a mathematically convenient representation of the text. Sometimes, people will use the terms interchangeably: for example, given some arbitrary document D, instead of saying “the vector that corresponds to document D”, they will just say “the vector D” or the “document D”. This achieves brevity at the cost of ambiguity.\r\n",
        "\r\n",
        "As long as you remember that documents exist in document space, and that vectors exist in vector space, the above ambiguity is acceptable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPaektMSKwZt"
      },
      "source": [
        "### Model\r\n",
        "\r\n",
        "Now that we have vectorized our corpus we can begin to transform it using models. We use model as an abstract term referring to a transformation from one document representation to another. In gensim documents are represented as vectors so a model can be thought of as a transformation between two vector spaces. The model learns the details of this transformation during training, when it reads the training Corpus.\r\n",
        "\r\n",
        "One simple example of a model is tf-idf. The tf-idf model transforms vectors from the bag-of-words representation to a vector space where the frequency counts are weighted according to the relative rarity of each word in the corpus.\r\n",
        "\r\n",
        "Here’s a simple example. Let’s initialize the tf-idf model, training it on our corpus and transforming the string “system minors”:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PffHeHDnK4Ns",
        "outputId": "95626f18-4bd9-405e-b859-2af5f852adb9"
      },
      "source": [
        "# 7.1\r\n",
        "from gensim import models\r\n",
        "\r\n",
        "# 7.2 train the model\r\n",
        "tfidf = models.TfidfModel(bag)\r\n",
        "\r\n",
        "# 7.3 Transform the \"system minors\" string:\r\n",
        "\r\n",
        "words = \"system minors\".lower().split()\r\n",
        "print(tfidf[dictionary.doc2bow(words)])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(6, 0.5898341626740045), (12, 0.8075244024440723)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOU7vsZdLNiT"
      },
      "source": [
        "The tfidf model again returns a list of tuples, where the first entry is the token ID and the second entry is the tf-idf weighting. Note that the ID corresponding to \"*system*\" (which occurred 4 times in the original corpus in three different documents ) has been weighted lower than the ID corresponding to \"*minors*\" (which only occurred twice in two different documents)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1cn7rbbOhg4"
      },
      "source": [
        "### Similarity of documents\r\n",
        "Once you’ve created the model, you can do all sorts of cool stuff with it. For example, to transform the whole corpus via TfIdf and index it, in preparation for similarity queries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Jwa3m7aOrDm"
      },
      "source": [
        "# 7.4\r\n",
        "from gensim import similarities\r\n",
        "\r\n",
        "# 7.5 Create a special data structure, 'index', \r\n",
        "#     to quickly get a similarity score\r\n",
        "\r\n",
        "index = similarities.SparseMatrixSimilarity(tfidf[bag], num_features=13)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2tI5xi6Ozd1"
      },
      "source": [
        "and to query the similarity of our query document query_document against every document in the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdc55n3oO06z",
        "outputId": "0a4bd619-b7b7-4451-bf9a-7f5cbc51db42"
      },
      "source": [
        "# 7.6 Document about which we want to calculate\r\n",
        "#     similarity\r\n",
        "query_document = 'system engineering'.split()\r\n",
        "\r\n",
        "# 7.6.1 Get Bag-of-words representation of this document\r\n",
        "query_bow = dictionary.doc2bow(query_document)\r\n",
        "\r\n",
        "# 7.6.1 Calculate similarity with\r\n",
        "#       with respect to each one\r\n",
        "#       of other document in the corpus\r\n",
        "\r\n",
        "sims = index[tfidf[query_bow]]\r\n",
        "print(list(enumerate(sims)))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 0.0), (1, 0.3086447), (2, 0.41707572), (3, 0.7184812), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9cCIfHkO6Xt"
      },
      "source": [
        "How to read this output?  \r\n",
        "\r\n",
        "Total Socuments: 9.  \r\n",
        "Document 3 has a similarity score of 0.718=72%,  \r\n",
        "Document 2 has a similarity score of 42% etc.   \r\n",
        "We can make this slightly more readable by sorting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emYjGBi-PAdz",
        "outputId": "8f54cf37-1f05-4bb2-e887-1f0bca5cb431"
      },
      "source": [
        "# 7.7 Sorted document similarity:\r\n",
        "\r\n",
        "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\r\n",
        "    print(document_number, score)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 0.7184812\n",
            "2 0.41707572\n",
            "1 0.3086447\n",
            "0 0.0\n",
            "4 0.0\n",
            "5 0.0\n",
            "6 0.0\n",
            "7 0.0\n",
            "8 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "bvytNic7tsi2",
        "outputId": "2e833c15-007c-43ab-8acc-1cdf5e2d84cb"
      },
      "source": [
        "\r\n",
        "#####################################\r\n",
        "\"\"\"\r\n",
        "Dictionaries in Python:\r\n",
        "=======================\r\n",
        "\r\n",
        "Module Collections has two types of dictioaries:\r\n",
        "i) OrderedDict and ii)\r\n",
        "\r\n",
        "OrderedDict in Python\r\n",
        "--------------------\r\n",
        "    An OrderedDict is a dictionary subclass that remembers\r\n",
        "    the order that keys were first inserted. The only\r\n",
        "    difference between dict() and OrderedDict() is that\r\n",
        "    OrderedDict preserves the order in which the keys are inserted.\r\n",
        "    A regular dict doesn’t track the insertion order, and iterating\r\n",
        "    it gives the values in an arbitrary order. By contrast, the order\r\n",
        "    the items are inserted is remembered by OrderedDict and are returned\r\n",
        "    in that order while iterating.\r\n",
        "\r\n",
        "defaultdict\r\n",
        "------------\r\n",
        "    A defaultdict works exactly like a normal dict, but it is initialized\r\n",
        "    with a function (called “default factory”) that takes no arguments\r\n",
        "    and provides the default value for a nonexistent key.\r\n",
        "    A defaultdict will never raise a KeyError. Any key that does not\r\n",
        "    exist gets the value returned by the default function.\r\n",
        "\r\n",
        "    from collections import defaultdict\r\n",
        "    # Create a defaultdict with an initialization\r\n",
        "    #  function:\r\n",
        "    ice_cream = defaultdict(lambda: 'Vanilla')\r\n",
        "    # Insert a key-value pair\r\n",
        "    ice_cream['Sarah'] = 'Chunky Monkey'\r\n",
        "    ice_cream['Sarah']    # Returns 'Chunky Monkey'\r\n",
        "    ice_cream['Joe']      # Key non-nonexistent. Returns Vanilla\r\n",
        "\r\n",
        "\"\"\""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nDictionaries in Python:\\n=======================\\n\\nModule Collections has two types of dictioaries:\\ni) OrderedDict and ii)\\n\\nOrderedDict in Python\\n--------------------\\n    An OrderedDict is a dictionary subclass that remembers\\n    the order that keys were first inserted. The only\\n    difference between dict() and OrderedDict() is that\\n    OrderedDict preserves the order in which the keys are inserted.\\n    A regular dict doesn’t track the insertion order, and iterating\\n    it gives the values in an arbitrary order. By contrast, the order\\n    the items are inserted is remembered by OrderedDict and are returned\\n    in that order while iterating.\\n\\ndefaultdict\\n------------\\n    A defaultdict works exactly like a normal dict, but it is initialized\\n    with a function (called “default factory”) that takes no arguments\\n    and provides the default value for a nonexistent key.\\n    A defaultdict will never raise a KeyError. Any key that does not\\n    exist gets the value returned by the default function.\\n\\n    from collections import defaultdict\\n    # Create a defaultdict with an initialization\\n    #  function:\\n    ice_cream = defaultdict(lambda: 'Vanilla')\\n    # Insert a key-value pair\\n    ice_cream['Sarah'] = 'Chunky Monkey'\\n    ice_cream['Sarah']    # Returns 'Chunky Monkey'\\n    ice_cream['Joe']      # Key non-nonexistent. Returns Vanilla\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfK1t1n6twJt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQkKqDJbLqkh"
      },
      "source": [
        "from gensim import similarities\r\n",
        "\r\n",
        "index = similarities.SparseMatrixSimilarity(tfidf[bag], num_features=13)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "-k1Hh6BlLtzx",
        "outputId": "7f4a9737-3955-46e9-bea1-04b3e53595aa"
      },
      "source": [
        "index.get_similarities()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-021b01b54970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_similarities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: get_similarities() missing 1 required positional argument: 'query'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPENGzECLvKE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}