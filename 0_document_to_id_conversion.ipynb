{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0.document_to_id_conversion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMDZZ7Nrivg9tw8vmlrIIF2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/deeplearning-sequences/blob/main/0_document_to_id_conversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8l27MZ8pkSr"
      },
      "source": [
        "# Last amended: 06th March, 2021\r\n",
        "# Ref: https://radimrehurek.com/gensim/tut1.html#from-strings-to-vectors\r\n",
        "#      https://www.tutorialspoint.com/gensim/gensim_creating_a_bag_of_words_corpus.htm\r\n",
        "#\r\n",
        "# Objective:\r\n",
        "#            Using gensim\r\n",
        "#         A. Convert tokens with each document to corresponding\r\n",
        "#            'token-ids' or integer-tokens.\r\n",
        "#            For text cleaning, pl refer wikiclustering file\r\n",
        "#            in folder: 10.nlp_workshop/text_clustering\r\n",
        "#            This file uses gensim for tokenization\r\n",
        "#         B. Keras also has  Tokenizer class that can also be\r\n",
        "#            used for integer-tokenization. See file:\r\n",
        "#            8.rnn/3.keras_tokenizer_class.py\r\n",
        "#         C. nltk can also tokenize. See file:\r\n",
        "#            10.nlp_workshop/word2vec/nlp_workshop_word2vec.py\r\n",
        "\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6foXDbN1Buy"
      },
      "source": [
        "The core concepts are:  \r\n",
        "**Document**  \r\n",
        "A document is an object of the text sequence type (commonly known as str in Python 3). A document could be anything from a short 140 character tweet, a single paragraph (i.e., journal article abstract), a news article, or a book.  \r\n",
        "`document = \"Human machine interface for lab abc computer applications\"`\r\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYogxQOG--X5"
      },
      "source": [
        "**Corpus**  \r\n",
        "A corpus is a collection of Document objects. Corpora serve two roles in Gensim:\r\n",
        "1.  Input for training a Model. During training, the models use this training corpus to look for common themes and topics, initializing their internal model parameters.\r\n",
        "\r\n",
        "2.  Gensim focuses on unsupervised models so that no human intervention, such as costly annotations or tagging documents by hand, is required.\r\n",
        "\r\n",
        "3. Documents to organize. After training, a topic model can be used to extract topics from new documents (documents not seen in the training corpus).\r\n",
        "\r\n",
        "4.  Such corpora can be indexed for Similarity Queries, queried by semantic similarity, clustered etc.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByawbP_e-4HS",
        "outputId": "db188e49-d8ef-4403-d5d8-8c6aa9fdaf53"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (4.2.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "w507D0iwpoaj",
        "outputId": "e783fdf4-299f-400b-c36d-65f66068c55c"
      },
      "source": [
        "\r\n",
        "#%reset -f\r\n",
        "\r\n",
        "# 1.1  gensim contains tools for Natural Language Processing\r\n",
        "#      Module 'corpora' contains sub-modules and methods to\r\n",
        "#      work with text documents\r\n",
        "from gensim import corpora\r\n",
        "\r\n",
        "# 1.2 defaultdict is like an ordinary dict. Only that if a key does\r\n",
        "#     not exist in the dict, then on its search it inserts that 'key'\r\n",
        "#     (as if that key existed)with a value that is defined by an \r\n",
        "#     initialization function (such as int()). \r\n",
        "#     See at the end of code: 'Dictionaries in Python'\r\n",
        "\r\n",
        "from collections import defaultdict\r\n",
        "\r\n",
        "# 1.3\r\n",
        "from gensim.utils import simple_preprocess\r\n",
        "\r\n",
        "# 1.4 To unnest a list of lists\r\n",
        "from gensim.utils import flatten\r\n",
        "\r\n",
        "# 1.5 pprint does pretty printing\r\n",
        "import pprint\r\n",
        "\r\n",
        "import gensim\r\n",
        "gensim.__version__"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.6.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OWqnbhaC4Ss"
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\r\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\r\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REiBOguLpt2e"
      },
      "source": [
        "\r\n",
        "# 2. Here is an example corpus. It consists of \r\n",
        "#    9 documents, where each document is a string\r\n",
        "#    consisting of a single sentence. \r\n",
        "#    Create a sample collection (list) of documents\r\n",
        "#    See text_clustering.py file as to how to get this list\r\n",
        "#    from folder of files or pandas dataframe:\r\n",
        "\r\n",
        "#    The first string is a paragraph having 3 sentences\r\n",
        "\r\n",
        "text_corpus = [ \r\n",
        "                \"Human machine interface for lab computer applications. \\\r\n",
        "                 Use it at abc. OK.\",\r\n",
        "                 \"A survey at of user opinion of computer system response time\",\r\n",
        "                \"The EPS user interface management system\",\r\n",
        "                \"System and human system engineering testing of EPS\",\r\n",
        "                \"Relation of user perceived response time to error measurement\",\r\n",
        "                \"The generation of random binary unordered trees\",\r\n",
        "                \"The intersection graph of paths in trees\",\r\n",
        "                \"Graph minors IV Widths of trees and well at quasi ordering\",\r\n",
        "                \"Graph minors A survey\"\r\n",
        "                ]\r\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kIKbm8SAPIv"
      },
      "source": [
        "Note  \r\n",
        "The above example loads the entire corpus into memory. In practice, corpora may be very large, so loading them into memory may be impossible. Gensim intelligently handles such corpora by streaming them one document at a time. See Corpus Streaming – One Document at a Time for details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbBMgnFdA4yA"
      },
      "source": [
        "Pre-processing  \r\n",
        "After collecting our corpus, there are typically a number of preprocessing steps we want to undertake. We’ll keep it simple and just remove some commonly used English words (such as ‘the’) and words that occur only once in the corpus. In the process of doing so, we’ll tokenize our data. Tokenization breaks up the documents into words (in this case using space as a delimiter)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "craIWA2rBAAY"
      },
      "source": [
        "# 2.1 Clean documents: See file text_clustering.py\r\n",
        "# 2.2 Stem documents : See file text_clustering.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE0ZhnOEp_4C"
      },
      "source": [
        "# 2.3 Lowercase each document, split it by white space and filter out stopwords\r\n",
        "\r\n",
        "# 2.3.1\r\n",
        "#     Create some list of stopwords that we do not want\r\n",
        "#     Detailed list of english stopwords is available at:\r\n",
        "#     https://gist.github.com/sebleier/554280\r\n",
        "\r\n",
        "#     We are not including the word 'at' here:\r\n",
        "\r\n",
        "stoplist = set('for a of the and to in'.split())\r\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZH4Q3hDqHAt"
      },
      "source": [
        "# 3. Define our own tokenize function.\r\n",
        "#     This function parses list of strings into\r\n",
        "#      list of words for each element or document\r\n",
        "#       in the document-collection\r\n",
        "def tokenize(docs):\r\n",
        "    tokenized = []          # Ist List: This will be a list of lists\r\n",
        "    for document in docs:   # For each senetence in the document-collection\r\n",
        "        tokenized_document = []  # IInd list: List of words per string or document\r\n",
        "        for word in document.lower().split():\r\n",
        "            if word not in stoplist:\r\n",
        "                tokenized_document.append(word)  # Append it to a list\r\n",
        "        tokenized.append(tokenized_document)         # Append list of words to a list\r\n",
        "    return tokenized\r\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CeyP3-gqMVN"
      },
      "source": [
        "texts = tokenize(text_corpus)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHyIu5YDqQZc",
        "outputId": "0e78f2dd-3955-43fb-b199-cae35f1d8343"
      },
      "source": [
        "\r\n",
        "pprint.pprint(texts)               #  List of list. The inner list\r\n",
        "                    #  contains tokens of respective documents\r\n",
        "len(texts)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['human',\n",
            "  'machine',\n",
            "  'at',\n",
            "  'interface',\n",
            "  'lab',\n",
            "  'abc',\n",
            "  'computer',\n",
            "  'applications'],\n",
            " ['survey', 'at', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
            " ['eps', 'user', 'interface', 'management', 'system'],\n",
            " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
            " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
            " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
            " ['intersection', 'graph', 'paths', 'trees'],\n",
            " ['graph',\n",
            "  'minors',\n",
            "  'iv',\n",
            "  'widths',\n",
            "  'trees',\n",
            "  'well',\n",
            "  'at',\n",
            "  'quasi',\n",
            "  'ordering'],\n",
            " ['graph', 'minors', 'survey']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMYkkoiSw6Jx",
        "outputId": "133dbd3b-bd96-4171-dd51-94d39d5b4048"
      },
      "source": [
        "# Should you like, you can \r\n",
        "# unnest the list of lists:\r\n",
        "ft = flatten(texts)\r\n",
        "print(ft)\r\n",
        "vocab = set(ft)\r\n",
        "print(\"\\n---Vocab-----\\n\")\r\n",
        "print(vocab)\r\n",
        "print(\"\\n---str length-----\\n\")\r\n",
        "len(ft)   # 55\r\n",
        "print(\"\\n---vocab length-----\\n\")\r\n",
        "len(vocab)  # 36"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['human', 'machine', 'at', 'interface', 'lab', 'abc', 'computer', 'applications', 'survey', 'at', 'user', 'opinion', 'computer', 'system', 'response', 'time', 'eps', 'user', 'interface', 'management', 'system', 'system', 'human', 'system', 'engineering', 'testing', 'eps', 'relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement', 'generation', 'random', 'binary', 'unordered', 'trees', 'intersection', 'graph', 'paths', 'trees', 'graph', 'minors', 'iv', 'widths', 'trees', 'well', 'at', 'quasi', 'ordering', 'graph', 'minors', 'survey']\n",
            "\n",
            "---Vocab-----\n",
            "\n",
            "{'opinion', 'measurement', 'machine', 'system', 'eps', 'iv', 'at', 'response', 'survey', 'computer', 'generation', 'lab', 'ordering', 'graph', 'minors', 'trees', 'widths', 'abc', 'random', 'applications', 'engineering', 'binary', 'unordered', 'quasi', 'interface', 'intersection', 'paths', 'time', 'error', 'human', 'well', 'management', 'relation', 'user', 'perceived', 'testing'}\n",
            "\n",
            "---str length-----\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---vocab length-----\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOQFGKeMqTwQ",
        "outputId": "5287ab08-8364-41f6-d62e-5a414bda7959"
      },
      "source": [
        "\r\n",
        "# 3.1 The following code is equivalent to above nested for-loops\r\n",
        "#     There being one list comprehension between another, output\r\n",
        "#     is not one list (as in ordinary list comprehension) but list\r\n",
        "#     within list.\r\n",
        "texts = [\r\n",
        "         [word  for word in document.lower().split(' ') if word not in stoplist]\r\n",
        "         for document in text_corpus \r\n",
        "        ]\r\n",
        "\r\n",
        "texts"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['human',\n",
              "  'machine',\n",
              "  'at',\n",
              "  'interface',\n",
              "  'lab',\n",
              "  'abc',\n",
              "  'computer',\n",
              "  'applications'],\n",
              " ['survey', 'at', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
              " ['eps', 'user', 'interface', 'management', 'system'],\n",
              " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
              " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
              " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
              " ['intersection', 'graph', 'paths', 'trees'],\n",
              " ['graph',\n",
              "  'minors',\n",
              "  'iv',\n",
              "  'widths',\n",
              "  'trees',\n",
              "  'well',\n",
              "  'at',\n",
              "  'quasi',\n",
              "  'ordering'],\n",
              " ['graph', 'minors', 'survey']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgRdTFHwRsvR"
      },
      "source": [
        "# Convert a document into a list of lowercase tokens,\r\n",
        " # ignoring tokens that are too short or too long.\r\n",
        " #  Uses tokenize() internally.\r\n",
        " texts = []\r\n",
        " for doc in text_corpus:\r\n",
        "   inner = []\r\n",
        "   out = simple_preprocess(doc, min_len=3, max_len=15)\r\n",
        "   outer.append(out)\r\n",
        "\r\n",
        " texts    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNUueHokqXjP"
      },
      "source": [
        "\r\n",
        "# 4.\r\n",
        "# Ref : https://www.ludovf.net/blog/python-collections-defaultdict/\r\n",
        "#  A defaultdict is just like a regular Python dict,\r\n",
        "#  except that it supports an additional argument at\r\n",
        "#  initialization: a function. If someone attempts to\r\n",
        "#  access a key to which no value has been assigned,\r\n",
        "#  that function will be called (without arguments)\r\n",
        "#  and its return value is used as the default value\r\n",
        "#  for the key.\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfR9hVOWqa0o",
        "outputId": "67eebff6-2157-42be-8f9e-308569e48af2"
      },
      "source": [
        "# 4.1 Initialise and create an empty dictionary\r\n",
        "#     by name of 'frequency'\r\n",
        "int()          # This function gives 0.\r\n",
        "               # Use it in defaultdict\r\n",
        "frequency = defaultdict(int)   # defaultdict(int) => key-values are int\r\n",
        "                               # defaultdict(list) => key-values are lists\r\n",
        "                               # Example: {'a' :['xx','yy'], 'b':['zz']}\r\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AinFJrDxqgAx"
      },
      "source": [
        "\r\n",
        "# 4.2 Get count of each word in the 'documents'\r\n",
        "# for every list in lists\r\n",
        "for doc in texts:        \r\n",
        "    # for every word in the inner list  \r\n",
        "    for token in doc:\r\n",
        "    \t# frequency[token] will first add a key 'token' to dict\r\n",
        "    \t#  (if the 'key' does not already exit) holding value '0'.\r\n",
        "    \t#   In either case value of the key will be incremented by 1\r\n",
        "    \t# So after all the loop is completed, value of each key\r\n",
        "    \t# will show its frequency\r\n",
        "        frequency[token] += 1\r\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpD6BJKKqjad",
        "outputId": "bceaf9e3-f7fb-4b71-ee16-be48eae47ce8"
      },
      "source": [
        "\r\n",
        "print(frequency)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<class 'int'>, {'human': 2, 'machine': 1, 'at': 3, 'interface': 2, 'lab': 1, 'abc': 1, 'computer': 2, 'applications': 1, 'survey': 2, 'user': 3, 'opinion': 1, 'system': 4, 'response': 2, 'time': 2, 'eps': 2, 'management': 1, 'engineering': 1, 'testing': 1, 'relation': 1, 'perceived': 1, 'error': 1, 'measurement': 1, 'generation': 1, 'random': 1, 'binary': 1, 'unordered': 1, 'trees': 3, 'intersection': 1, 'graph': 3, 'paths': 1, 'minors': 2, 'iv': 1, 'widths': 1, 'well': 1, 'quasi': 1, 'ordering': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz5_2zhDql-w"
      },
      "source": [
        "# 4.3 Remove words that appear only once\r\n",
        "#     So we create another list of lists\r\n",
        "#     texts = [['he','he','to'],['to','go']]\r\n",
        "#     frequency={'he' : 2, 'to': 2, 'go': 1}\r\n",
        "\r\n",
        "processed_corpus = list([])       # outer list\r\n",
        "# 4.3.1 For every list in the the\r\n",
        "#       outer list \r\n",
        "for doc in texts:\r\n",
        "  #  4.3.2 A blank list of tokens\r\n",
        "  #        with higher frequency\r\n",
        "\ttokens = list([]) \r\n",
        "  # 4.3.3 For every word in this \r\n",
        "  #       inner list   \r\n",
        "\tfor word in doc:\r\n",
        "\t\tif frequency[word] > 1:\r\n",
        "\t\t\ttokens.append(word)\r\n",
        "\tprocessed_corpus.append(tokens)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uo9dqnpPQtK",
        "outputId": "04c56012-1d67-4883-82c8-5f310ecd2a76"
      },
      "source": [
        "processed_corpus"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['human', 'at', 'interface', 'computer'],\n",
              " ['survey', 'at', 'user', 'computer', 'system', 'response', 'time'],\n",
              " ['eps', 'user', 'interface', 'system'],\n",
              " ['system', 'human', 'system', 'eps'],\n",
              " ['user', 'response', 'time'],\n",
              " ['trees'],\n",
              " ['graph', 'trees'],\n",
              " ['graph', 'minors', 'trees', 'at'],\n",
              " ['graph', 'minors', 'survey']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xodqzno_JrmF",
        "outputId": "6ca156de-7102-493a-8a4b-3e2b3a6c2cc9"
      },
      "source": [
        "# The above is equivalent to the following:\r\n",
        "#  Only keep words that appear more than once\r\n",
        "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\r\n",
        "pprint.pprint(processed_corpus)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['human', 'at', 'interface', 'computer'],\n",
            " ['survey', 'at', 'user', 'computer', 'system', 'response', 'time'],\n",
            " ['eps', 'user', 'interface', 'system'],\n",
            " ['system', 'human', 'system', 'eps'],\n",
            " ['user', 'response', 'time'],\n",
            " ['trees'],\n",
            " ['graph', 'trees'],\n",
            " ['graph', 'minors', 'trees', 'at'],\n",
            " ['graph', 'minors', 'survey']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7NhFUB1qpxs",
        "outputId": "3942eb76-ac5d-45c7-9f39-ec472babf1f2"
      },
      "source": [
        "\r\n",
        "# 4.4\r\n",
        "print(processed_corpus)   #     output = [['he','he','to'],['to']]\r\n",
        "print(texts)              # Compare the above with this\r\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n",
            "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'], ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'management', 'system'], ['system', 'human', 'system', 'engineering', 'testing', 'eps'], ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'], ['generation', 'random', 'binary', 'unordered', 'trees'], ['intersection', 'graph', 'paths', 'trees'], ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'], ['graph', 'minors', 'survey']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vfxZ1iiQt4g",
        "outputId": "d725517f-06ce-48ef-ddeb-7377de8463a1"
      },
      "source": [
        "text_corpus"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Human machine at interface for lab abc computer applications',\n",
              " 'A survey at of user opinion of computer system response time',\n",
              " 'The EPS user interface management system',\n",
              " 'System and human system engineering testing of EPS',\n",
              " 'Relation of user perceived response time to error measurement',\n",
              " 'The generation of random binary unordered trees',\n",
              " 'The intersection graph of paths in trees',\n",
              " 'Graph minors IV Widths of trees and well at quasi ordering',\n",
              " 'Graph minors A survey']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGhwsNqoIoER"
      },
      "source": [
        "Before proceeding further, we want to associate each word in the corpus with a unique integer ID. We can do this using the gensim.corpora.Dictionary class. This dictionary defines the vocabulary of all words that our processing knows about."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEMkFL5TqsK-"
      },
      "source": [
        "\r\n",
        "# 5. Module 'corpora.Dictionary' implements the concept of\r\n",
        "#     Dictionary – a mapping between words and their integer ids.\r\n",
        "#    Ref: https://radimrehurek.com/gensim/corpora/dictionary.html\r\n",
        "dictionary = corpora.Dictionary(processed_corpus)\r\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2voEDOHqvDw",
        "outputId": "5dd29b2e-90b7-403f-ffe4-ff9ffa612cc5"
      },
      "source": [
        "\r\n",
        "# 5.1\r\n",
        "dictionary      # Just informs where it is stroed in memory\r\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.corpora.dictionary.Dictionary at 0x7f1217db0fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojv-mvGYqxOt",
        "outputId": "df1ce240-18b2-43a3-ba4c-78c5ff94e8f5"
      },
      "source": [
        "\r\n",
        "# 5.2\r\n",
        "print(dictionary.token2id)      # Another function is id2token\r\n",
        " "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSmJOfFcqzz5",
        "outputId": "796158ea-7254-450e-ecf2-6db948a69350"
      },
      "source": [
        "\r\n",
        "# 5.3 Convert document into the bag-of-words (bow)\r\n",
        "#      format ie list of (integer-tokens, token_count) per document.\r\n",
        "#      Bag of sequences does not give integer sequences in the order\r\n",
        "#      they are in the sentence but in increasing order of integer values.\r\n",
        "#      Thus, it is just a bag.\r\n",
        "bag = [dictionary.doc2bow(text) for text in output]\r\n",
        "bag\r\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1)],\n",
              " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
              " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
              " [(1, 1), (5, 2), (8, 1)],\n",
              " [(3, 1), (6, 1), (7, 1)],\n",
              " [(9, 1)],\n",
              " [(9, 1), (10, 1)],\n",
              " [(9, 1), (10, 1), (11, 1)],\n",
              " [(4, 1), (10, 1), (11, 1)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly58Py3Xq3gC",
        "outputId": "10c22346-57f9-416f-c8f4-fa2750459461"
      },
      "source": [
        "\r\n",
        "# 5.4 Just seperate integer-tokens from frequency\r\n",
        "id_text=[]\r\n",
        "for doc in bag:\r\n",
        "    doclist=[]\r\n",
        "    for id,_ in doc:\r\n",
        "        doclist.append(id)\r\n",
        "    id_text.append(doclist)\r\n",
        "\r\n",
        "id_text"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 1, 2],\n",
              " [0, 3, 4, 5, 6, 7],\n",
              " [2, 5, 7, 8],\n",
              " [1, 5, 8],\n",
              " [3, 6, 7],\n",
              " [9],\n",
              " [9, 10],\n",
              " [9, 10, 11],\n",
              " [4, 10, 11]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "bvytNic7tsi2",
        "outputId": "2e833c15-007c-43ab-8acc-1cdf5e2d84cb"
      },
      "source": [
        "\r\n",
        "#####################################\r\n",
        "\"\"\"\r\n",
        "Dictionaries in Python:\r\n",
        "=======================\r\n",
        "\r\n",
        "Module Collections has two types of dictioaries:\r\n",
        "i) OrderedDict and ii)\r\n",
        "\r\n",
        "OrderedDict in Python\r\n",
        "--------------------\r\n",
        "    An OrderedDict is a dictionary subclass that remembers\r\n",
        "    the order that keys were first inserted. The only\r\n",
        "    difference between dict() and OrderedDict() is that\r\n",
        "    OrderedDict preserves the order in which the keys are inserted.\r\n",
        "    A regular dict doesn’t track the insertion order, and iterating\r\n",
        "    it gives the values in an arbitrary order. By contrast, the order\r\n",
        "    the items are inserted is remembered by OrderedDict and are returned\r\n",
        "    in that order while iterating.\r\n",
        "\r\n",
        "defaultdict\r\n",
        "------------\r\n",
        "    A defaultdict works exactly like a normal dict, but it is initialized\r\n",
        "    with a function (called “default factory”) that takes no arguments\r\n",
        "    and provides the default value for a nonexistent key.\r\n",
        "    A defaultdict will never raise a KeyError. Any key that does not\r\n",
        "    exist gets the value returned by the default function.\r\n",
        "\r\n",
        "    from collections import defaultdict\r\n",
        "    # Create a defaultdict with an initialization\r\n",
        "    #  function:\r\n",
        "    ice_cream = defaultdict(lambda: 'Vanilla')\r\n",
        "    # Insert a key-value pair\r\n",
        "    ice_cream['Sarah'] = 'Chunky Monkey'\r\n",
        "    ice_cream['Sarah']    # Returns 'Chunky Monkey'\r\n",
        "    ice_cream['Joe']      # Key non-nonexistent. Returns Vanilla\r\n",
        "\r\n",
        "\"\"\""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nDictionaries in Python:\\n=======================\\n\\nModule Collections has two types of dictioaries:\\ni) OrderedDict and ii)\\n\\nOrderedDict in Python\\n--------------------\\n    An OrderedDict is a dictionary subclass that remembers\\n    the order that keys were first inserted. The only\\n    difference between dict() and OrderedDict() is that\\n    OrderedDict preserves the order in which the keys are inserted.\\n    A regular dict doesn’t track the insertion order, and iterating\\n    it gives the values in an arbitrary order. By contrast, the order\\n    the items are inserted is remembered by OrderedDict and are returned\\n    in that order while iterating.\\n\\ndefaultdict\\n------------\\n    A defaultdict works exactly like a normal dict, but it is initialized\\n    with a function (called “default factory”) that takes no arguments\\n    and provides the default value for a nonexistent key.\\n    A defaultdict will never raise a KeyError. Any key that does not\\n    exist gets the value returned by the default function.\\n\\n    from collections import defaultdict\\n    # Create a defaultdict with an initialization\\n    #  function:\\n    ice_cream = defaultdict(lambda: 'Vanilla')\\n    # Insert a key-value pair\\n    ice_cream['Sarah'] = 'Chunky Monkey'\\n    ice_cream['Sarah']    # Returns 'Chunky Monkey'\\n    ice_cream['Joe']      # Key non-nonexistent. Returns Vanilla\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfK1t1n6twJt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}